#!/bin/sh

# exit if any command fails
set -e


# look for the config file in the usual places
# first try the environment
configpath=$(realpath "$PYP_CONFIG" || echo "")
defaultpath=~/.pyp/config.toml
if [ -z "$configpath" ] || [ ! -f "$configpath" ] ; then
  # then look in the default location
  configpath=$defaultpath
fi
if [ -z "$configpath" ] || [ ! -f "$configpath" ] ; then
  echo "no configuration file found!"
  echo "checked PYP_CONFIG=$PYP_CONFIG"
  echo "and $defaultpath"
  exit 1
fi
echo "Using configuration at $configpath"

# set the actual config path to another environment variable, so all the downstream things can find it easily
export PYP_CONFIG_ACTUAL=$configpath

# read the developer arguments, if any
if [ $# -ge 1 ] ; then
  devpath=$1
  devversion=$2
fi

# this shell script doesn't know how to parse the TOML file
# but the Kotlin code inside the container does
# so use the container to parse the config file and check for errors
# but send the directories back here to the shell script for validation
# since the conainer can't check external directories without binds, which we haven't done yet

# create binds to map the config file into the container
# must bind the config file to a canonical path so it doesn't conflict with any of the bind folders
binds="--bind $configpath:/var/micromon/config.toml"

# handle development binds if needed
if [ -n "$devpath" ] ; then
  echo "looking for development files at \"$devpath\" with version $devversion ..."

  libspath=$devpath/build/image/libs
  if [ -d "$libspath" ] ; then
    echo "using libraries folder at: $libspath"
    binds="$binds --bind $libspath:/opt/micromon/libs"
  else
    echo "Error: libs folder not found at \"$libspath\""
    exit 1
  fi

  jarpath=$devpath/build/libs/micromon.jar
  if [ "$devversion" != "unspecified" ] ; then
    jarpath=$devpath/build/libs/micromon-$devversion.jar
  fi
  if [ -f "$jarpath" ] ; then
    echo "using jar at: $jarpath"
    jarname=$(basename "$jarpath")
    binds="$binds --bind $jarpath:/opt/micromon/libs/$jarname"
  else
    echo "Error: jar not found at \"$jarpath\""
    exit 1
  fi

  classpathpath=$devpath/build/classpath.txt
  if [ -f "$classpathpath" ] ; then
    echo "using classpath at: $classpathpath"
    classpathname=$(basename "$classpathpath")
    binds="$binds --bind $classpathpath:/opt/micromon/bin/$classpathname"
  else
    echo "Error: classpath not found at \"$classpathpath\""
    exit 1
  fi

  initpath=$devpath/config/init.sh
  if [ -f "$initpath" ] ; then
    echo "using init script at: $initpath"
    binds="$binds --bind $initpath:/opt/micromon/init.sh"
  else
    echo "Error: init script not found at \"$initpath\""
    exit 1
  fi

  startpath=$devpath/config/micromon.sh
    if [ -f "$startpath" ] ; then
      echo "using JVM start script at: $startpath"
      binds="$binds --bind $startpath:/opt/micromon/bin/micromon.sh"
    else
      echo "Error: JVM start script not found at \"$startpath\""
      exit 1
    fi

  clipath=$devpath/config/cli.sh
    if [ -f "$clipath" ] ; then
      echo "using CLI script at: $clipath"
      binds="$binds --bind $clipath:/opt/micromon/bin/cli.sh"
    else
      echo "Error: CLI script not found at \"$clipath\""
      exit 1
    fi
fi

# use the pyp args config too, if it's available
argspath=$PYP_SRC/config/pyp_config.toml
if [ -f "$argspath" ] ; then
  echo "using PYP args at: $argspath"
  binds="$binds --bind $argspath:/opt/micromon/pyp_config.toml"
fi

# build the command to run the CLI
cli="singularity run --app cli $binds nextPYP.sif"

echo "Reading config.toml using CLI tool ..."

# get the local and share directories from the config
localdir=$($cli localdir)
shareddir=$($cli shareddir)

# make the directories if needed
if [ ! -d "$localdir" ] ; then
  echo "Creating local directory at: $localdir"
  mkdir -p "$localdir"
fi

logsdir="$localdir/logs"
if [ ! -d "$logsdir" ] ; then
  echo "Creating logs directory at: $logsdir"
  mkdir -p "$logsdir"
fi

dbdir="$localdir/db"
if [ ! -d "$dbdir" ] ; then
  echo "Creating database directory at: $dbdir"
  mkdir -p "$dbdir"
fi

if [ ! -d "$shareddir" ] ; then
  echo "Creating shared directory at: $shareddir"
  mkdir -p "$shareddir"
fi

# add all the binds from the config file
configbinds=$($cli binds)
binds="$binds --bind $localdir --bind $shareddir $configbinds"


if [ -z "$PYP_HOSTPROCESSOR_OFF" ]; then

  pidpath="/tmp/micromon.hostprocessor.pid"
  pipepath="/tmp/micromon.hostprocessor.pipe"

  # clean files if explicitly asked
  if [ -n "$PYP_HOSTPROCESSOR_CLEAN" ]; then
    echo "Cleaning any previous hostprocessor files"
    rm "$pidpath" 2> /dev/null || true
    rm "$pipepath" 2> /dev/null || true
  fi

  # start the host processor on a named pipe, so the container can launch processes on the host OS
  if [ -f "$pidpath" ]; then
    echo "A host processor is already running. Stop it before starting Micromon."
    echo "And then delete the files $pidpath and $pipepath"
    exit 1
  fi
  mkfifo "$pipepath"
  chmod go-rwx "$pipepath"

  binds="$binds --bind $pipepath"

  # get the host processor executable
  if [ -n "$devpath" ]; then
    # dev mode, look in the dev folder
    hpexec="$devpath/run/hostprocessor"
  else
    # production mode, look in /usr/bin
    if [ -x /usr/bin/nextpyp-hostprocessor ]; then
      hpexec="/usr/bin/nextpyp-hostprocessor"
    else
      hpexec="./nextpyp-hostprocessor"
    fi
  fi
  if [ ! -x "$hpexec" ]; then
    echo "Host processor executable not found at $hpexec"
    exit 1
  fi

  # actually start the host processor
  "$hpexec" > "$localdir/logs/hostprocessor" 2>&1 &
fi


echo "Configuring environment ..."

# configure the environment
NEXTPYP_LOCAL=$localdir
export NEXTPYP_LOCAL
NEXTPYP_HEAPMIB=$($cli heapmib)
export NEXTPYP_HEAPMIB
NEXTPYP_JMX=$($cli jmx)
export NEXTPYP_JMX
NEXTPYP_DATABASE_MEMGB=$($cli database_memgb)
export NEXTPYP_DATABASE_MEMGB
NEXTPYP_OOMDUMP=$($cli oomdump)
export NEXTPYP_OOMDUMP


# The website tries to query the number of available GPUs in standalone mode
# so we need to get apptainer to bind in the GPU libraries.
# We can just always ask apptainer to do it whether or not Cuda is even available on this host,
# but apptainer will show scary warnings in the console if it's not.
# Ideally, we'd only turn on --nv if we knew Cuda was available, but detecting that in a shell script
# is likely to be imperfect, so just always turn it on and hope no one complains about the apptainer warnings.
gpus=--nv


# finally, start the container
echo "Starting singularity container ..."
echo "NOTE:    If you see warnings immediately below that nv files or libraries can't be found, those warnings are safe to ignore."
singularity instance start $binds --writable-tmpfs --no-home $gpus nextPYP.sif nextPYP
